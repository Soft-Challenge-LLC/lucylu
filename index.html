<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Lucy Lu, Ph.D.</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon fa-gem"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Lucy Lu, Ph.D.</h1>
								<p>

								</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#intro">Intro</a></li>
								<li><a href="#work">Work</a></li>
								<li><a href="#about">About</a></li>
								<li><a href="#contact">Contact</a></li>
								<!--<li><a href="#elements">Elements</a></li>-->
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">
								<h2 class="major">Intro</h2>
								<span class="image main"><img src="images/pic01.jpg" alt="" /></span>
								<h3>Education:</h3>
								<ul>
									<li>University of South Florida, Computer Science. Ph. D, 2016-2023</li>
									<li>University of South Florida, MUMA Business School. MBA, 2018-2022</li>
									<li>Tsinghua University, Computer Science. B.S, 2002</li>
									<li>Tsinghua University, Computer Science. M.S, 2005</li>
								</ul>
									<h3>Certificates:</h3>
								<ul>
									<li>Data Science Fellow, The Data Incubaot, 2020</li>
									<li>Google Analytics for Beginners, 2017</li>
									<li>Advanced Google Analytics, 2017</li>
									<li>MIT Big Data and Social Behavior, 2018</li>
								</ul>
								<h3>Skills:</h3>
								<ul>
									<li>Programming Languages: Python, R, Java, C, VBA, SQL</li>
									<li>Database: Oracle, Microsoft SQL Server, MySQL, Postgres</li>
									<li>Information System: SAP, ArcGIS, Google Analytics</li>
									<li>Data Analysis: Machine Learning, Data Modeling, Data Quality Control, Data Visualization</li>
									<li>Expertise: Text Mining, Image Processing, Social Network Analysis</li>
								</ul>
									<h3>Professional Work:</h3>
								<ul>
									<li>Employer: National Center for Toxicological Research, January, 2006-December, 2015</li>
									<li>Employer: Soft Challenge LLC, Tampa, Florida, February, 2012-now</li>
								</ul>
								<h3>Internship Work:</h3>
								<ul>
									
									<li>TCM Bank LLC, February, 2020 – April, 2020</li>
									<li>credit risk modeling and analysis.</li>
									<li>operational risk modeling and analysis,</li>
									<li>market risk modeling and analysis,</li>
									<li>fraud risk modeling and analysis.</li>
									<li>Executive Management Report Automation and Delivery.</li>
								</ul>
								<ul>
									<li>Quality Counts LLC, March, 2019- August, 2020</li>
									<li>traffic detection</li>
									<li>travel planning</li>
									<li>congestion detection</li>
									<li>transportation planning</li>
									<li>curve safety analysis</li>
								</ul>
								<ul>
									<li>Geographic Solutions, January, 2018 – June, 2019</li>
									<li>population group and interpretation</li>
									<li>Key performance indicator (KPI) definition and evaluation
									located based job market analysis</li>
								</ul>
									<h3>Awards:</h3>
								<ul>
									<li>Best Paper Award, at WMSCI 2016 Conference, for the paper titled “Combining Bayesian and Semantic Analysis with Domain Knowledge”</li>
									<li>Best Paper Award, at WMSCI 2013, for the paper titled “Discovery of Strong Association Rules for Attributes from Data for Program of All-Inclusive Care for the Elderly (PACE)”</li>
									<li>Best Poster Award, at National Science Foundation (NSF) Bioinformatics Workshop to Foster Collaborative Research 2013, for the poster titled “Linkage Discovery with Glossaries and Topic Segmentation”</li>
									<li>Microsoft Innovation Cup Student Software Contest, 1st Place. 2005</li>
									<li>Best Paper Award, at WMSCI 2011 Conference, for the paper titled “Statistical Quality Control of Microarray Gene Expression Data”</li>
									<li>BEA Scholarship, 2005</li>
									<li>Guanghua Scholarship, 2004</li>
									<li>Tsinghua Scholarship, 2003</li>
								</ul>
							</article>

						<!-- Work -->
							<article id="work">
								<h2 class="major">Projects</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
<p><div id="top">
In image processing, we focus on edge detection and object detection. For edge detection, we use Conditional Random Fields (CRF) to improve the existing edge detection methdology. We detect all edges with closed lines and without duplicate edges. For object detection, we add arbitrary features to the objects to deep learning model to enforce deep learning model to learn the characteristics of specific objects.
<ul>
	<li><a href="#edge-detection">Edge Detection</a></li>
	<li><a href="#object-detection">Object Detection</a></li>
</ul>
</div>
<div id="edge-detection">
<h2>Edge Detection</h2>
The challenge in line segmentation is that the threshold of the gradients have to be determined in the first place, a set of criteria  are needed to define the properties of line segments,  fractions of the image need to be detected, the validation of the line segment is also determined arbitrarily. We proposed a sematic line segmentation detection model based on conditional random fields (CRF) model. Based on the gradients of the image, this methodology does not require any prior knowledge about the image, no fraction selection, no threshold, no criteria for region growth and rectangle validation. 
<h2>Methodology</h2>
CRF model, as shown in figure 1, can be considered as Markov random fields model. The initial transition probability of each pixel is the same. During each iteration, the state function can average out the different among neighbors and the transition function update the featured pixels. Apparently, state function make the local pixels similar to each other. This operation is especially useful when some pixels are changed because of noise or system errors. When the significance of each pixel is computed not only with the value of the pixel itself but also the values of the adjacent neighbors, system errors and arbitrary noise can be average out. Feature function is defined with the features we pick so that it can consistently increase the significance of features and depress the significance of none features. 
<div style="text-align:center">
<img style="margin-left:auto;margin-right:auto;" src="images/CRF.png"></img>
<p>Figure 1. Conditional Random Fields Model</p>
</div>
<h2>Performance</h2>
We tested the methodology with several images, such as, the image of simple lines, the image of simple objects, and the portrait of a person. We use the same feature functions and the same Gaussian kernel to detection line segments in three different images. In figure 2(a)(b)(c), we compared out results with the ones generated with Linear Time Line Segmentation Detection (LTLSD) method.
<div style="text-align: center">
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-lsd-lines.png"></img>
<p>(a)</p>
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-lsd-chairs.png"></img>
<p>(b)</p>
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-lsd-lena.png"></img>
<p>(c)</p>
<p>Figure 2. Comparison of CRF results with LTLSD results</p>
</div>
<p>In LTLSD result, as shown in figure 2(a), objects in the image are not closed. As shown in figure 2(b), many line segments are doubled which make the image not readable, some lines apparently are shadows but were extracted as lines. As shown in figure 2(c), in the background, a lot of lines are cut. Around the face and the hat, some small circles and short lines are not detected and some are not closed.
<p><a href="#top">Top</a></p>
</div>
<div id="object-detection">
<h2>Object Detection</h2>
Deep learning is generally used in image recognition. For example, deep learning can recognize handwriting of 10 digital numbers in MNIST data set. Deep learning does not require pre-selected feature set for training, as shown in figure 1. During training, deep learning model repeatedly select features, evaluate the quality of the feature set and generate output for next layer. However, What exactly does the deep learning model learn from the training set?
<div style="text-align:center">
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-deeplearning.png"></img>
<p>Figure 1. Deep Learning Model</p>
</div>
In image processing, there are a lot of features in one image. It is information rich. Normally, one small detail is the combination of many pixels, which is too much for human to quantitatively check each one of the computation units - the pixels. 

Deep learning model selects features from entire images. If there are some common patterns which can differentiate the ten categories, those patterns can be chosen as the feature set. Some features are related to the pictures, and some features are related to the objects, but, if we don't enforce the deep learning model to learn from objects, most of the features deep learning model learned are based on the pictures, which means deep learning model does not really learn the objects. This is major flaw in this process. If we change the background of the objects, deep learning can not recognize the objects any more.
<h2>Methodology</h2>
We add feature function to deep learning model so that deep learning model can be enforced to learn from the objects only, as shown in figure 2. 
<div style="text-align:center">
<img style="margin-left:auto;margin-right:auto;" src="images/ip-deeplearning-enforced.png"></img>
<p>Figure 2. Feature Enforced Deep Learning Model</p>
</div>
<h2>Performance</h2>
<div style="text-align: center">
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-2layer-original.png"></img>
<p>(a) Output from Original Deep Learning Model</p>
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-2layer-enforced.png"></img>
<p>(b) Output from Feature Enforced Deep Learning Model</p>
<p>Figure 2. Comparison of Features in Decision layer</p> 
</div>
In the original output in figure 2(a), we can see some feature spots which are the common patterns in different images but those patterns are not meaningful. After we add feature filter to the deep learning model, the model only picks features from object regions and the output shows the features of the objects, as shown in figure 2(b). 
<p><a href="#top">Top</a></p>
</div>
</p>
<p>

<div>
<h2>Technology Trends</h2>
<p>
Understanding the structure of the network is central in network analysis. We are interested in extracting valuable information from the network, such as trends, popularity, dynamic of the trends, and also profiles of popular items.

This prediction model can be used for many different applications. such as 
<ul>
	<li>Technology Trends: citation/patent network</li>
	<li>Online market: co-purchase network</li>
	<li>Social Media Trends: online news/live journal/media sharing network</li>
	<li>Social Dynamics: friends network</li>
</ul>

</p>
<h2>Sample Data</h2>
This is a citation graph for high-energy physics research papers from 1991 to 2000, with a total of N = 29,555 papers and E = 352,807 citations
<p></p>
<h2>Data Modeling</h2>
<p>In terms of data modeling, we need to define popular topics for each year with the citation data and also find out how many are new topics. In each popular topic, we need to find density of the topic, the change of the density in ten years, the diameter of the topic and the change of the diameter in ten years. 

Lots of features are unknown in the citation network. For example, we don’t know the related topics of the papers. we don’t know how to define the popularity because we don’t have a big picture of the citation network. We can start from any paper and search along the path through the entire network. Whatever we can find is highly related to where we started. The citation network is so big that it is not feasible to try all of the papers. Also, we know that greedy search in the network is NP-hard. It is impossible to get it done in polynomial time.
</p>
<h2>Methodology</h2>
<p>We decompose the network into two layers, as shown in figure 1. We use kronecker graph in figure 2(a) as the computation unit. Kronecker graph is the smallest symmetric graph and also is the smallest community in social network. Symmetric graph has structural properties and the mathematical properties which can support the decomposition and reconstruction. </p>
<div style="text-align:center" >
<img style="margin-left:auto;margin-right:auto;" src="images/interesting-layer.png"></img>
<br><p>Figure 1. Network Decomposition</p>
<img style="margin-left:auto;margin-right:auto;" src="images/kronecker-graph.png"></img>
<br><p>Figure 2(a). Kronecker Graph</p>
<img style="margin-left:auto;margin-right:auto;" src="images/kronecker-join.png"></img>
<br><p>Figure 2(b). Graph Join and Project</p>
</div>
<h2>Results</h2>
<p>
From 1991 to 2000, we summarized the number of nodes (papers and references), the number of edges (citations), maximum number of times that a paper was cited, the averages number of times that a paper was cited, as shown in figure 3. </p>
<div style="text-align:center">
<img style="margin-left:auto;margin-right:auto;" src="images/stats-citation.png"></img>
<br><b>Figure 3. Statistics of Co-citation from 1991 to 2000</b>
</div>
<p>Each co-citation defines a topic so that the frequent co-citations define the popular topics. The frequency of the co-citation is highly biased. We use Expectation Maximization (EM) to group co-citations into 3 clusters, and statistics of popular topics is shown in figure 4. From 1991 to 2000, the number of publications, citations, and co-citations increase. From 1991 to 1995, there are less publications, citations, and co-citations. After 1995, the increase of publications, citations, and co-citations is not as fast as those years from 1991 to 1995. The number of topics increases from 1991 to 1994. After 1994, the change in the number of topics each year is small. Sometimes, it goes up and sometimes, it goes down, and it is around the average of 800 topics each year. </p>
<div style="text-align:center">
<img style="margin-left:auto;margin-right:auto;" src="images/stats-topics.png"></img>
<br><b>Figure 4. Statistics of Popular Topics from 1991 to 2000</b>
</div>
<p>
These trends of research in10 years indicate that technology evolves every five years and can be completely updated in 10 years. When we work the frontier research, we need to trace back for about 5 years. In more than 5 years, most of the research is about something else. The trends of new publications, citation, and co-citations are consistent which can prove each other.
</p>
<p>The number of topics increase from 1991 to 1994. In 1991, there are no paper published about the popular topics in 2000. From 1995 to 2000, the number of topics is almost stable. the number of topics goes up and down but the changes are small.  The number of topics is consistent with the number of publications, the number of citations, the number of co-citations. </p>

<div style="text-align:center">
<img style="margin-left:auto;margin-right:auto;" src="images/network-topics.png"></img>
<br><b>Figure 5. Citation Network on Popular Topics from 1992 to 2000</b>
</div>
</div>	
</p>
<p>

   

<div class="container">
	<script src="js/libraries/jquery/jquery.min.js"></script>
    <script src="js/libraries/bootstrap/bootstrap.min.js"></script>
    <script src="js/libraries/d3/d3.min.js"></script>
    <script src="js/script.js"></script>
</div>

<div>
<h2>Online News Popularity Prediction</h2>
<p>
We have online news from three different websites: Facebook, GooglePlus and LinkedIn. We want to find out, for any news, if we can predict its popularity, based on the popularity of the existing news. 

This prediction model can be used for many different applications. such as 
<ul>
	<li>advertising</li>
	<li>election campaign</li>
	<li>posts recommendation</li>
	<li>dynamic content management</li>
</ul>

</p>

<h2>Sample Data</h2>
<table style="border: 1px solid black; width:auto; height:auto;">
	<tr><td>IDLink</td><td>Title</td><td>Headline</td><td>Source</td><td>Topic</td><td>PublishDate</td><td>SentimentTitle</td><td>SentimentHeadline</td><td>Facebook</td><td>GooglePlus</td><td>LinkedIn</td></tr>
	<tr><td>88518</td><td>Israel denies permits to Gazans for Palestine Marathon</td><td>Israel denies permits to Gazans for Palestine Marathon. A Palestinian youth stands next to a national flag at the Palestinian side of Beit Hanoun</td><td>The Daily Star</td><td>palestine</td><td>3/31/2016  3:45:16 PM</td><td>0.100503782</td><td>0.150351633</td><td>24</td><td>5</td><td>2</td></tr>
	<tr><td>87218</td><td>Local organizations join USDA for initiative to bolster region's ...</td><td>Local organizations join USDA for initiative to bolster region's economy. Story &middot; Comments. Print: Create a hardcopy of this page; Font Size:</td><td>Bristol Herald Courier (press release) (blog)</td><td>economy</td><td>3/31/16 15:54</td><td>-0.180421959</td><td>0.0875</td><td>12</td><td>8</td><td>0</td></tr>
</table>
</div>

<div>
<h2>Data Modeling</h2>
<p>
Based on this data, we can see it is both a text mining task and also a classification task. We need to understand text and also use other features to label each data entry. The data model is built in the following steps:

<ul>
	<li>Build semantic space</li>
	<li>Generate topics which can be used to define news</li>
	<li>Generate sample set for a given news</li>
	<li>Remove Outliers</li>
	<li>KNN Classification</li>
	<li>Adjust parameters for better results</li>
	<li>Interpret results</li>
</ul>
</p>
<p>
	For text mining, we can have two solutions
	<ul>
		<li>Bag of words</li>
		<li>Latent Semantic Analysis (LSA)</li>
	</ul>
	For popularity prediction, we choose Latent semantic analysis. 
</p>
<p>
	For other features, such as Source, SentimentTitle, SentimentHeadline, we can directly use them to make prediction. 
	Before we use choose classification algoritm, we evaluate the significance of the features with information gain, as listed below:
	<table style="border: 1px solid black">
		<tr><td>Website</td><td>Source</td><td>SentimentTitle</td><td>SentimentHeadline</td></tr>
		<tr><td>Facebook</td><td>0.6427983828436333></td><td>0.682137527933072</td><td>0.6065400706568865</td></tr>
		<tr><td>GooglePlus</td><td>0.4545358118900283></td><td>0.48596147409162366</td><td>0.4117329019171993</td></tr>
		<tr><td>LinkedIn</td><td>0.5177361250103715</td><td>0.5575869048549802</td><td>0.4672467847269911</td></tr>
	</table>
	The information gain shows use that these features are not very strongly related to the prediction. We don't have many features to choose. We don't have domain knowledge to add extra information into the feature set, either. We need to improve the quality of the raw data as much as possible.

	Our solution is that we not only use latent semantic analysis to make the text feature more meaningful, also, based on the new data, we only use similar news to predict the label of the new data. In other words, we create a specific data set for each news we are going to classify. The data modeling process can be generalized but each model we generated is specified for the a particular news. We don't create a general model for all of the news, but, each model can classify the particular news better.

</p>

</div>

<div>
<h2>Latent Semantic Analysis</h2>
<p>
	Latent semantic analysis (LSA) is a technique for text processing. The idea behind latent semantic analysis is that, in psychology, we consider people learn language through words instead of grammar. Think about how babies learn languages. They just put words together and it is going to make sense. However, the context of the words can not be ignored, although we can ignore the grammar. 
	Each word can have multiple meanings in different context and different words can have the same meanings in the same context. This is the most confusing part in text mining, which almost makes it impossible to process the text. However, when LSA comes into play, the myth is deciphered. The way it works is straightforward that it simplifies the content. The formula of LSA is listed below.
</p>
	<image style="width:80%;height:100%;border-width:1px;left:20px;position:relative;" src="images/LSA.png" ></image>
<p>
	The matrix in the middle on the right side of equal symbol is for topics. We start with all the text and it ends up with we project text into several topics and thos topics are related to the text content.
</p>
</div>

<div>
<h2>Performance</h2>
<p>
We test the data modeling strategy for three times with three different news. The performance is listed below. 
</p>
<h2>Experiment 1</h2>
<table>
	<tr><td>News</td><td>Get a $50 Microsoft Store gift card with Xbox </td></tr>
	<tr><td>Dataset size</td><td>7942</td></tr>
	<tr><td>After Sample Selection</td><td>212</td></tr>
	<tr><td>Features</td><td>Source, SentimentTitle, SentimentHeadline</td></tr>
	<tr><td>Predict</td><td>Popularity</td></tr>
</table>

<table style="border: 1px solid black">
	<tr><td>Source</td><td>Actual Score</td><td>Predicted Score</td><td>Average</td><td>Max</td><td>Min</td></tr>
	<tr><td>Facebook</td><td>9</td><td>10</td><td>64.5</td><td>3832</td><td>0</td></tr>
	<tr><td>GooglePlus</td><td>2</td><td>3</td><td>7.58</td><td>186</td><td>0</td></tr>
	<tr><td>LinkedIn</td><td>1</td><td>1</td><td>22.7</td><td>438</td><td>0</td></tr>
</table>
</div>

<div>
<h2>Experiment 2</h2>
<table>
	<tr><td>News</td><td>Windows 10 Mobile Build 10586.306 now being ...</td></tr>
	<tr><td>Dataset size</td><td>7942</td></tr>
	<tr><td>After Sample Selection</td><td>82</td></tr>
	<tr><td>Features</td><td>Source, SentimentTitle, SentimentHeadline</td></tr>
	<tr><td>Predict</td><td>Popularity</td></tr>
</table>
<table style="border: 1px solid black">
	<tr><td>Source</td><td>Actual Score</td><td>Predicted Score</td><td>Average</td><td>Max</td><td>Min</td></tr>
	<tr><td>Facebook</td><td>23</td><td>3</td><td>60.5</td><td>1055</td><td>0</td></tr>
	<tr><td>GooglePlus</td><td>0</td><td>1</td><td>6.8</td><td>234</td><td>0</td></tr>
	<tr><td>LinkedIn</td><td>0</td><td>0</td><td>22.8</td><td>433</td><td>0</td></tr>
</table>

<h2>Experiment 3</h2>
<table>
	<tr><td>News</td><td>Xbox One Backward Compatibility: Two new games ...</td></tr>
	<tr><td>Dataset size</td><td>7942</td></tr>
	<tr><td>After Sample Selection</td><td>87</td></tr>
	<tr><td>Features</td><td>Source, SentimentTitle, SentimentHeadline</td></tr>
	<tr><td>Predict</td><td>Popularity</td></tr>
</table>
<table style="border: 1px solid black">
	<tr><td>Source</td><td>Actual Score</td><td>Predicted Score</td><td>Average</td><td>Max</td><td>Min</td></tr>
	<tr><td>Facebook</td><td>6</td><td>5</td><td>31.2</td><td>775</td><td>0</td></tr>
	<tr><td>GooglePlus</td><td>0</td><td>0</td><td>5.4</td><td>181</td><td>0</td></tr>
	<tr><td>LinkedIn</td><td>0</td><td>5</td><td>24.7</td><td>622</td><td>0</td></tr>
</table>

</p>
</div>
</p>
							</article>

						<!-- About -->
							<article id="about">
								<h2 class="major">About</h2>
								<span class="image main"><img src="images/pic03.jpg" alt="" /></span>
								<p>Lucy Lu is a data scientist. Her research interest is in Insights Analysis, Natural Language Processing, Entity Resolution, Deep Learning and Image Processing. She has been providing machine learning solutions in different research areas. Her research solutions have won best paper awards for three times. With her experiences in technical details, her solutions can achieve almost perfect results.</p>
							</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Lucy Lu, Ph.D.</label>
										</div>
										<div class="field half">
											<label for="email">mollielu2012@gmail.com</label>
										</div>
									</div>
								</form>
								<ul class="icons">
									<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</article>


					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Design: <a href="https://html5up.net">Lucy Lu</a>.</p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
