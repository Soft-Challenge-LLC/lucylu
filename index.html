<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Lucy Lu, Ph.D.</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon fa-gem"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Lucy Lu, Ph.D.</h1>
								<p>

								</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#intro">Intro</a></li>
								<li><a href="#work">Work</a></li>
								<li><a href="#about">About</a></li>
								<li><a href="#contact">Contact</a></li>
								<!--<li><a href="#elements">Elements</a></li>-->
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">
								<h2 class="major">Intro</h2>
								<span class="image main"><img src="images/pic01.jpg" alt="" /></span>
								<h3>Education:</h3>
								<ul>
									<li>University of South Florida, Computer Science. Ph. D, 2016-2023</li>
									<li>Tsinghua University, Computer Science. B.S, 2002</li>
									<li>Tsinghua University, Computer Science. M.S, 2005</li>
								</ul>
									<h3>Certificates:</h3>
								<ul>
									<li>Certified Data Scientist, The Data Incubator, 2020</li>
									<li>Google Analytics for Beginners, 2017</li>
									<li>Advanced Google Analytics, 2017</li>
									<li>MIT Big Data and Social Behavior, 2018</li>
									<li>IBM Advanced Machine Learing and Signal Processing, 2023</li>
								</ul>
								<h3>Skills:</h3>
								<ul>
									<li>Programming Languages: Python, R, Java, C, VBA, SQL</li>
									<li>Database: Oracle, Microsoft SQL Server, MySQL, Postgres</li>
									<li>Information System: SAP, ArcGIS, Google Analytics</li>
									<li>Data Analysis: Machine Learning, Data Modeling, Data Quality Control, Data Visualization</li>
									<li>Expertise: Text Mining, Image Processing, Social Network Analysis</li>
								</ul>
									<h3>Professional Work:</h3>
								<ul>
									<li>Employer: National Center for Toxicological Research, January, 2006-December, 2015</li>
									<li>Employer: Soft Challenge LLC, Tampa, Florida, February, 2012-now</li>
								</ul>
								<h3>Internship Work:</h3>
								<ul>
									<li>Verinovum Inc, October, 2022 – March, 2023</li>
									<li>Data Cleaning,</li>
									<li>Entity Resolution for Lab Tests, Medication, and Other Medical Terms in Billion Doctor's Notes with 100% Accuracy</li>
								</ul>
								<ul>
									<li>Financial Information Technologies LLC, October, 2021 – August, 2022</li>
									<li>Data Collection through Web Crawling,</li>
									<li>Entity Resolution for Packing Styles, Brands, and Products in Billion Invoices with 100% Accuracy,</li>
									<li>Snowflake Data Warehouse Management,</li>
									<li>fProduct/Nonproduct Classification for Billion Invoices.</li>
								</ul>
								<ul>
									
									<li>TCM Bank LLC, February, 2020 – April, 2020</li>
									<li>credit risk modeling and analysis.</li>
									<li>operational risk modeling and analysis,</li>
									<li>market risk modeling and analysis,</li>
									<li>fraud risk modeling and analysis.</li>
									<li>Executive Management Report Automation and Delivery.</li>
								</ul>
								<ul>
									<li>Quality Counts LLC, March, 2019- August, 2020</li>
									<li>traffic detection</li>
									<li>travel planning</li>
									<li>congestion detection</li>
									<li>transportation planning</li>
									<li>curve safety analysis</li>
								</ul>
								<ul>
									<li>Geographic Solutions, January, 2018 – June, 2019</li>
									<li>population group and interpretation</li>
									<li>Key performance indicator (KPI) definition and evaluation
									located based job market analysis</li>
								</ul>
									<h3>Awards:</h3>
								<ul>
									<li>Best Paper Award, at WMSCI 2016 Conference, for the paper titled “Combining Bayesian and Semantic Analysis with Domain Knowledge”</li>
									<li>Best Paper Award, at WMSCI 2013, for the paper titled “Discovery of Strong Association Rules for Attributes from Data for Program of All-Inclusive Care for the Elderly (PACE)”</li>
									<li>Best Poster Award, at National Science Foundation (NSF) Bioinformatics Workshop to Foster Collaborative Research 2013, for the poster titled “Linkage Discovery with Glossaries and Topic Segmentation”</li>
									<li>Microsoft Innovation Cup Student Software Contest, 1st Place. 2005</li>
									<li>Best Paper Award, at WMSCI 2011 Conference, for the paper titled “Statistical Quality Control of Microarray Gene Expression Data”</li>
									<li>BEA Scholarship, 2005</li>
									<li>Guanghua Scholarship, 2004</li>
									<li>Tsinghua Scholarship, 2003</li>
								</ul>
							</article>

						<!-- Work -->
							<article id="work">
								<h2 class="major">Projects</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
<div id="top">
	<ul>
		<li><a href="#image-processing">Image Processing</a>
			<ol style="list-style-type: lower-alpha; padding-bottom: 0;">
			  <li style="margin-left:2em"><a href="#edge-detection">Edge Detection</a></li>
			  <li style="margin-left:2em; padding-bottom: 0;"><a href="#object-detection">Object Detection</a></li>
			 </ol>
		</li>
		<li><a href="#text-mining">Text Analysis</a>
			 <ol style="list-style-type: lower-alpha; padding-bottom: 0;">
			  <li style="margin-left:2em"><a href="#online-news">Online News Popularity Prediction</a></li>
			  <li style="margin-left:2em; padding-bottom: 0;"><a href="#text-classification">Featured Transformer Model (FTM) for Text Classification</a></li>
			  <li style="margin-left:2em"><a href="#entity-detection">Entity Extraction through Conditional Random Fields model (CRF)</a></li>
			 </ol>
		</li>
		<li><a href="#social-graph">Social Graph Analysis</a></li>
		<li><a href="#fraud-detection">Credit Card Fraud Detection</a></li>
	</ul>
</div>


<div id='image-processing'>
<h2>Image Processing</h2>
<p>
	In image processing, we focus on edge detection and object detection. For edge detection, we use Conditional Random Fields (CRF) to improve the existing edge detection methdology. We detect all edges with closed lines and without duplicate edges. For object detection, we add arbitrary features to the objects to deep learning model to enforce deep learning model to learn the characteristics of specific objects.
</p>
</div>

<div id="edge-detection">
<h3>Edge Detection</h3>
<p>
	The challenge in line segmentation is that the threshold of the gradients have to be determined in the first place, a set of criteria  are needed to define the properties of line segments,  fractions of the image need to be detected, the validation of the line segment is also determined arbitrarily. We proposed a sematic line segmentation detection model based on conditional random fields (CRF) model. Based on the gradients of the image, this methodology does not require any prior knowledge about the image, no fraction selection, no threshold, no criteria for region growth and rectangle validation. 
</p>
<h5>Methodology</h5>
<p>
	CRF model, as shown in figure 1, can be considered as Markov random fields model. The initial transition probability of each pixel is the same. During each iteration, the state function can average out the different among neighbors and the transition function update the featured pixels. Apparently, state function make the local pixels similar to each other. This operation is especially useful when some pixels are changed because of noise or system errors. When the significance of each pixel is computed not only with the value of the pixel itself but also the values of the adjacent neighbors, system errors and arbitrary noise can be average out. Feature function is defined with the features we pick so that it can consistently increase the significance of features and depress the significance of none features. 
</p>
<img style="margin-left:auto;margin-right:auto;" src="images/CRF.png"></img>
<p>Figure 1. Conditional Random Fields Model</p>
<h5>Performance</h5>
<p>
	We tested the methodology with several images, such as, the image of simple lines, the image of simple objects, and the portrait of a person. We use the same feature functions and the same Gaussian kernel to detection line segments in three different images. In figure 2(a)(b)(c), we compared out results with the ones generated with Linear Time Line Segmentation Detection (LTLSD) method.
</p>

<img style="margin-left:auto;margin-right:auto;"  src="images/ip-lsd-lines.png"></img>
<p>(a)</p>
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-lsd-chairs.png"></img>
<p>(b)</p>
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-lsd-lena.png"></img>
<p>(c)</p>
<p>Figure 2. Comparison of CRF results with LTLSD results</p>

<p>In LTLSD result, as shown in figure 2(a), objects in the image are not closed. As shown in figure 2(b), many line segments are doubled which make the image not readable, some lines apparently are shadows but were extracted as lines. As shown in figure 2(c), in the background, a lot of lines are cut. Around the face and the hat, some small circles and short lines are not detected and some are not closed.
<p><a href="#top">Top</a></p>
</div>

<div id="object-detection">
<h3>Object Detection</h3>
<p>
Deep learning is generally used in image recognition. For example, deep learning can recognize handwriting of 10 digital numbers in MNIST data set. Deep learning does not require pre-selected feature set for training, as shown in figure 1. During training, deep learning model repeatedly select features, evaluate the quality of the feature set and generate output for next layer. However, What exactly does the deep learning model learn from the training set?</p>

<img style="margin-left:auto;margin-right:auto;"  src="images/ip-deeplearning.png"></img>
<p>Figure 1. Deep Learning Model</p>

<p>
	In image processing, there are a lot of features in one image. It is information rich. Normally, one small detail is the combination of many pixels, which is too much for human to quantitatively check each one of the computation units - the pixels. 
	<br>
	<br>
	Deep learning model selects features from entire images. If there are some common patterns which can differentiate the ten categories, those patterns can be chosen as the feature set. Some features are related to the pictures, and some features are related to the objects, but, if we don't enforce the deep learning model to learn from objects, most of the features deep learning model learned are based on the pictures, which means deep learning model does not really learn the objects. This is major flaw in this process. If we change the background of the objects, deep learning can not recognize the objects any more.
</p>
<h5>Methodology</h5>
<p>We add feature function to deep learning model so that deep learning model can be enforced to learn from the objects only, as shown in figure 2.</p>

<img style="margin-left:auto;margin-right:auto;" src="images/ip-deeplearning-enforced.png"></img>
<p>Figure 2. Feature Enforced Deep Learning Model</p>

<h5>Performance</h5>

<img style="margin-left:auto;margin-right:auto;"  src="images/ip-2layer-original.png"></img>
<p>(a) Output from Original Deep Learning Model</p>
<img style="margin-left:auto;margin-right:auto;"  src="images/ip-2layer-enforced.png"></img>
<p>(b) Output from Feature Enforced Deep Learning Model</p>
<p>Figure 3. Comparison of Features in Decision layer</p> 
<p>
	In the original output in figure 3(a), we can see some feature spots which are the common patterns in different images but those patterns are not meaningful. After we add feature filter to the deep learning model, the model only picks features from object regions and the output shows the features of the objects, as shown in figure 3(b). 
</p>
<p><a href="#top">Top</a></p>
</div>


<div id='social-graph'>
<h2>Social Graph Analysis</h2>
<h3>Technology Trends</h3>
<p>
Understanding the structure of the network is central in network analysis. We are interested in extracting valuable information from the network, such as trends, popularity, dynamic of the trends, and also profiles of popular items.
<br>
<br>
This prediction model can be used for many different applications. such as 
	<ul>
		<li>Technology Trends: citation/patent network</li>
		<li>Online market: co-purchase network</li>
		<li>Social Media Trends: online news/live journal/media sharing network</li>
		<li>Social Dynamics: friends network</li>
	</ul>
</p>

<h5>Sample Data</h5>
<p>
	This is a citation graph for high-energy physics research papers from 1991 to 2000, with a total of N = 29,555 papers and E = 352,807 citations
</p>
<h5>Data Modeling</h5>
<p>
	In terms of data modeling, we need to define popular topics for each year with the citation data and also find out how many are new topics. In each popular topic, we need to find density of the topic, the change of the density in ten years, the diameter of the topic and the change of the diameter in ten years. 
	<br>
	<br>
	Lots of features are unknown in the citation network. For example, we don’t know the related topics of the papers. we don’t know how to define the popularity because we don’t have a big picture of the citation network. We can start from any paper and search along the path through the entire network. Whatever we can find is highly related to where we started. The citation network is so big that it is not feasible to try all of the papers. Also, we know that greedy search in the network is NP-hard. It is impossible to get it done in polynomial time.
</p>
<h5>Methodology</h5>
<p>
	We decompose the network into two layers, as shown in figure 1. We use kronecker graph in figure 2(a) as the computation unit. Kronecker graph is the smallest symmetric graph and also is the smallest community in social network. Symmetric graph has structural properties and the mathematical properties which can support the decomposition and reconstruction. 
</p>

<img style="margin-left:auto;margin-right:auto;" src="images/interesting-layer.png"></img>
<p><b>Figure 1. Network Decomposition</b></p>
<img style="margin-left:auto;margin-right:auto;" src="images/kronecker-graph.png"></img>
<p><b>Figure 2(a). Kronecker Graph</b></p>
<img style="margin-left:auto;margin-right:auto;" src="images/kronecker-join.png"></img>
<p><b>Figure 2(b). Graph Join and Project</b></p>

<h5>Results</h5>
<p>
From 1991 to 2000, we summarized the number of nodes (papers and references), the number of edges (citations), maximum number of times that a paper was cited, the averages number of times that a paper was cited, as shown in figure 3. </p>

<img style="margin-left:auto;margin-right:auto;" src="images/stats-citation.png"></img>
<p><b>Figure 3. Statistics of Co-citation from 1991 to 2000</b></p>

<p>
	Each co-citation defines a topic so that the frequent co-citations define the popular topics. The frequency of the co-citation is highly biased. We use Expectation Maximization (EM) to group co-citations into 3 clusters, and statistics of popular topics is shown in figure 4. From 1991 to 2000, the number of publications, citations, and co-citations increase. From 1991 to 1995, there are less publications, citations, and co-citations. After 1995, the increase of publications, citations, and co-citations is not as fast as those years from 1991 to 1995. The number of topics increases from 1991 to 1994. After 1994, the change in the number of topics each year is small. Sometimes, it goes up and sometimes, it goes down, and it is around the average of 800 topics each year. 
</p>

<img style="margin-left:auto;margin-right:auto;" src="images/stats-topics.png"></img>
<p><b>Figure 4. Statistics of Popular Topics from 1991 to 2000</b></p>

<p>
	These trends of research in10 years indicate that technology evolves every five years and can be completely updated in 10 years. When we work the frontier research, we need to trace back for about 5 years. In more than 5 years, most of the research is about something else. The trends of new publications, citation, and co-citations are consistent which can prove each other.
</p>
<p>
	The number of topics increase from 1991 to 1994. In 1991, there are no paper published about the popular topics in 2000. From 1995 to 2000, the number of topics is almost stable. the number of topics goes up and down but the changes are small.  The number of topics is consistent with the number of publications, the number of citations, the number of co-citations. 
</p>

<img style="margin-left:auto;margin-right:auto;" src="images/network-topics.png"></img>
<p><b>Figure 5. Citation Network on Popular Topics from 1992 to 2000</b></p>

<p><a href="#top">Top</a></p>
</div>


<div id='text-mining'>

<h2>Text Mining</h2>
<p>
	For text mining, we focus on text similarity through dimension reduction by using Latent Semantic Analysis (LSA), text classification by using Featured Transformer Model (FTM) on top of Bi-directional Encoder Representation Transformer (BERT), entity extraction by using Conditional Random Fields mode. (CRF).
	<ul>
		<li>Online News Popularity Prediction</li>
		<li>Text Classification through Featured Transformer Model (FTM)</li>
		<li>Entity Extraction through Conditional Random Fields model (CRF)</li>
	</ul>
</p>
</div>

<div id='online-news'>
<h3>Online News Popularity Prediction</h3>
<p>
	We have online news from three different websites: Facebook, GooglePlus and LinkedIn. We want to find out, for any news, if we can predict its popularity, based on the popularity of the existing news. 

	This prediction model can be used for many different applications. such as 
	<ul>
		<li>advertising</li>
		<li>election campaign</li>
		<li>posts recommendation</li>
		<li>dynamic content management</li>
	</ul>
</p>

<h5>Sample Data</h5>
<table style="border: 1px solid black; width:auto; height:auto;">
	<tr><td>Dimensions</td><td>Sample 1</td><td>Sample 2</td></tr>
	<tr><td>IDLink</td><td>88518</td><td>87218</td></tr>
	<tr><td>Title</td><td>Israel denies permits to Gazans for Palestine Marathon</td><td>Local organizations join USDA for initiative to bolster region's ...</td></tr>
	<tr><td>Headline</td><td>Israel denies permits to Gazans for Palestine Marathon. A Palestinian youth stands next to a national flag at the Palestinian side of Beit Hanoun</td><td>Local organizations join USDA for initiative to bolster region's economy. Story &middot; Comments. Print: Create a hardcopy of this page; Font Size:</td></tr>
	<tr><td>Source</td><td>The Daily Star</td><td>Bristol Herald Courier (press release) (blog)</td></tr>
	<tr><td>Topic</td><td>palestine</td><td>economy</td></tr>
	<tr><td>PublishDate</td><td>3/31/2016  3:45:16 PM</td><td>3/31/16 15:54</td></tr>
	<tr><td>SentimentTitle</td><td>0.100503782</td><td>-0.180421959</td></tr>
	<tr><td>SentimentHeadline</td><td>0.150351633</td><td>0.0875</td></tr>
	<tr><td>Facebook</td><td>24</td><td>12</td></tr>
	<tr><td>GooglePlus</td><td>5</td><td>8</td></tr>
	<tr><td>LinkedIn</td><td>2</td><td>0</td></tr>
	
</table>

<h5>Data Modeling</h5>
<p>
Based on this data, we can see it is both a text mining task and also a classification task. We need to understand text and also use other features to label each data entry. The data model is built in the following steps:

<ul>
	<li>Build semantic space</li>
	<li>Reduce dimensions with Latent Semantic Analysis</li>
	<li>Generate sample set for given news</li>
	<li>Remove Outliers</li>
	<li>KNN Classification</li>
	<li>Adjust parameters for better results</li>
	<li>Interpret results</li>
</ul>
</p>
<p>
	For text mining, we can have two solutions
	<ul>
		<li>Bag of words</li>
		<li>Latent Semantic Analysis (LSA)</li>
	</ul>
	For popularity prediction, we choose Latent semantic analysis. 
</p>
<p>
	For other features, such as Source, SentimentTitle, SentimentHeadline, we can directly use them to make prediction. 
	Before we use choose classification algoritm, we evaluate the significance of the features with information gain, as listed below:
	<table style="border: 1px solid black">
		<tr><td>Website</td><td>Source</td><td>SentimentTitle</td><td>SentimentHeadline</td></tr>
		<tr><td>Facebook</td><td>0.6427983828436333></td><td>0.682137527933072</td><td>0.6065400706568865</td></tr>
		<tr><td>GooglePlus</td><td>0.4545358118900283></td><td>0.48596147409162366</td><td>0.4117329019171993</td></tr>
		<tr><td>LinkedIn</td><td>0.5177361250103715</td><td>0.5575869048549802</td><td>0.4672467847269911</td></tr>
	</table>
	The information gain shows use that these features are not very strongly related to the prediction. We don't have many features to choose. We don't have domain knowledge to add extra information into the feature set, either. We need to improve the quality of the raw data as much as possible.
	<br>
	<br>
	Our solution is that we not only use latent semantic analysis to make the text feature more meaningful, also, based on the new data, we only use similar news to predict the label of the new data. In other words, we create a specific data set for each news we are going to classify. The data modeling process can be generalized but each model we generated is specified for the a particular news. We don't create a general model for all of the news, but, each model can classify the particular news better.

</p>

<h5>Latent Semantic Analysis</h5>
<p>
	Latent semantic analysis (LSA) is a technique for text processing. The idea behind latent semantic analysis is that, in psychology, we consider people learn language through words instead of grammar. Think about how babies learn languages. They just put words together and it is going to make sense. However, the context of the words can not be ignored, although we can ignore the grammar. 
	Each word can have multiple meanings in different context and different words can have the same meanings in the same context. This is the most confusing part in text mining, which almost makes it impossible to process the text. However, when LSA comes into play, the myth is deciphered. The way it works is straightforward that it simplifies the content. The formula of LSA is listed below.
</p>
	<image style="width:80%;height:100%;border-width:1px;left:20px;position:relative;" src="images/LSA.png" ></image>
	<p><b>Figure 1. Latent Semantic Analysis</b></p>
<p>
	The matrix in the middle on the right side of equal symbol is for topics. We start with all the text and it ends up with we project text into several topics and thos topics are related to the text content.
</p>

<h5>Performance</h5>
<p>
We test the data modeling strategy for three times with three different news. The performance is listed below. 
</p>
<h5>Experiment 1</h5>
<p>
<table>
	<tr><td>News</td><td>Get a $50 Microsoft Store gift card with Xbox </td></tr>
	<tr><td>Dataset size</td><td>7942</td></tr>
	<tr><td>After Sample Selection</td><td>212</td></tr>
	<tr><td>Features</td><td>Source, SentimentTitle, SentimentHeadline</td></tr>
	<tr><td>Predict</td><td>Popularity</td></tr>
</table>

<table style="border: 1px solid black">
	<tr><td>Source</td><td>Actual Score</td><td>Predicted Score</td><td>Average</td><td>Max</td><td>Min</td></tr>
	<tr><td>Facebook</td><td>9</td><td>10</td><td>64.5</td><td>3832</td><td>0</td></tr>
	<tr><td>GooglePlus</td><td>2</td><td>3</td><td>7.58</td><td>186</td><td>0</td></tr>
	<tr><td>LinkedIn</td><td>1</td><td>1</td><td>22.7</td><td>438</td><td>0</td></tr>
</table>
</p>
<h5>Experiment 2</h5>
<p>
<table>
	<tr><td>News</td><td>Windows 10 Mobile Build 10586.306 now being ...</td></tr>
	<tr><td>Dataset size</td><td>7942</td></tr>
	<tr><td>After Sample Selection</td><td>82</td></tr>
	<tr><td>Features</td><td>Source, SentimentTitle, SentimentHeadline</td></tr>
	<tr><td>Predict</td><td>Popularity</td></tr>
</table>
<table style="border: 1px solid black">
	<tr><td>Source</td><td>Actual Score</td><td>Predicted Score</td><td>Average</td><td>Max</td><td>Min</td></tr>
	<tr><td>Facebook</td><td>23</td><td>3</td><td>60.5</td><td>1055</td><td>0</td></tr>
	<tr><td>GooglePlus</td><td>0</td><td>1</td><td>6.8</td><td>234</td><td>0</td></tr>
	<tr><td>LinkedIn</td><td>0</td><td>0</td><td>22.8</td><td>433</td><td>0</td></tr>
</table>
</p>
<h5>Experiment 3</h5>
<p>
<table>
	<tr><td>News</td><td>Xbox One Backward Compatibility: Two new games ...</td></tr>
	<tr><td>Dataset size</td><td>7942</td></tr>
	<tr><td>After Sample Selection</td><td>87</td></tr>
	<tr><td>Features</td><td>Source, SentimentTitle, SentimentHeadline</td></tr>
	<tr><td>Predict</td><td>Popularity</td></tr>
</table>
<table style="border: 1px solid black">
	<tr><td>Source</td><td>Actual Score</td><td>Predicted Score</td><td>Average</td><td>Max</td><td>Min</td></tr>
	<tr><td>Facebook</td><td>6</td><td>5</td><td>31.2</td><td>775</td><td>0</td></tr>
	<tr><td>GooglePlus</td><td>0</td><td>0</td><td>5.4</td><td>181</td><td>0</td></tr>
	<tr><td>LinkedIn</td><td>0</td><td>5</td><td>24.7</td><td>622</td><td>0</td></tr>
</table>

</p>

<p><a href="#top">Top</a></p>
</div>

<div id='text-classification'>
<h3>Featured Transformer Model (FTM) for Text Classification</h3>
<p>
Natural language is the most common ways to present information in different fields, in different perspectives and in different circumstances so that text becomes a rich source of information. However, with the development of information technology, the amount of documentation becomes impossible for human to handle. We need to take advantage of computers to help automatically understand documentation, group documentation into different categories, topics and subjects until the total amount of information is capable for human to handle.
<br>
<br>
Attention mechanism is a big breakthrough in deep learning. It becomes an increasingly popular concept and a useful tool in developing deep learning models for Natural Language Processing (NLP). Because it builds a shortcut of the context for input texts and promotes correlated words in one sentence. The importance vector in the attention layer can be updated through Markov-like updates and can be customized easily in classification. The prediction can be made by estimating how strongly the word is correlated with or attends to other words.
<br>
<br>
We propose a Featured Transformer Methodology (FTM) based on attention mechanism. It can efficiently add domain knowledge to deep learning architecture. The attention mechanism performs Markov-like updates. When the model converts, the associations between domain features and word embeddings can be extracted.

</p>

<h5>Sample Data</h5>
<p>
<table style="border: 1px solid black">
	<tr><td>Samples</td><td>Class</td></tr>
	<tr><td>WBC        8.5        x/uL        (4.5-9.8)</td><td>LAB</td></tr>
	<tr><td>RBC        4.5        x/uL        (0.5-5.0)</td><td>LAB</td></tr>
	<tr><td>MCV        0.5        x/uL        (0.0-5.0)</td><td>LAB</td></tr>
	<tr><td>tramadol Allergy Intermediate hives</td><td>DOC</td></tr>
	<tr><td>codeine Allergy Mild NAUSEA</td><td>DOC</td></tr>
</table>
</p>

<h5>Methodology</h5>
<p>
	Bidirectional Encoder Representations from Transformers (BERT) has transformer network architecture. Attention layer plays the role of aligning encoder and decoder layers. Encoder and decoder layers are built upon convolution networks instead of recurrent networks to reduce computation costs. Attention layer can be updated through Markov-like process as shown in Figure 1. 
</p>
<img style="margin-left:auto;margin-right:auto;" src="images/HMMUpdate.png"></img>
<p><b>Figure 1. Markov Chain Updates</b></p>
<p>
Connections between two sequences are presented in two-dimensional matrix in which each dimension has a sequence of tokens. The more often the connections appear, the larger the values are. Attention mechanism is a shortcut of Markov Chain in which attention layer has less heads than the previous layer. As shown in figure 2, in BERT network architecture, the importance vector of attention layer can align encoder to decoder and highly affect the final prediction. If we can properly adjust importance vector, for example, to promote positive patterns and suppress negative/random patterns, we can efficiently improve the model performance.
<br>
<br>
We propose Featured Transformer Methodology (FTM) to improve the performance of Transformer networks, especially on BERT network architecture. The idea is that we define patterns according to the domain knowledge, add patterns as arbitrary features to the input layer and use arbitrary features to promote the importance of corresponding terms on attention layer. 
</p>

<h5>Performance</h5>
<p>
	The model was tested with 2871 samples in which 788 samples were labeled as 'lab' and 2083 samples were labeled as 'doc'. As shown in the following table, the experimental results showed that the performance measurements, such as precision, recall, and F1-score, indicated that the model can be used to identify lab tests with 100% in all measurements. In comparison with the Conditional Random Fields (CRF) model, Bi-directional Encoder Representation Transformer (BERT), Naïve Bayes Support Vector Machine (NBSVM), Logistic Regression (LOGREG), FASTTEXT, Standard Gated Recurrent Units (STANDARD GRU), Bi-directional Gated Recurrent Units (BiGRU), Featured Transformer Methodology (FTM) can perform much better than CRF, BERT, NBSVM, LOGREG, FASTTEXT, STANDARD GRU, and BiGRU in all measurements.
</p>
<p>
Table 1. Performance of FTM on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>1.00</td><td>1.00</td><td>1.00</td></tr>
	<tr><td>LAB</td><td>1.00</td><td>1.00</td><td>1.00</td></tr>
	<tr><td>Macro Avg</td><td>1.00</td><td>1.00</td><td>1.00</td></tr>
	<tr><td>Weighted Avg</td><td>1.00</td><td>1.00</td><td>1.00</td></tr>
</table>
</p>
<p>
Table 2. Performance of CRF on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.927</td><td>0.989</td><td>0.957</td></tr>
	<tr><td>LAB</td><td>0.959</td><td>0.764</td><td>0.850</td></tr>
	<tr><td>Macro Avg</td><td>0.943</td><td>0.876</td><td>0.904</td></tr>
	<tr><td>Weighted Avg</td><td>0.935</td><td>0.933</td><td>0.931</td></tr>
</table>
</p>
<p>
Table 3. Performance of BERT on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
	<tr><td>LAB</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
	<tr><td>Macro Avg</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
	<tr><td>Weighted Avg</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
</table>
</p>
<p>
Table 4. Performance of NBSVM on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.92</td><td>0.98</td><td>0.95</td></tr>
	<tr><td>LAB</td><td>0.99</td><td>0.97</td><td>0.98</td></tr>
	<tr><td>Macro Avg</td><td>0.96</td><td>0.97</td><td>0.96</td></tr>
	<tr><td>Weighted Avg</td><td>0.97</td><td>0.97</td><td>0.97</td></tr>
</table>
</p>
<p> 
Table 5. Performance of LOGREG on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.92</td><td>0.96</td><td>0.94</td></tr>
	<tr><td>LAB</td><td>0.98</td><td>0.97</td><td>0.97</td></tr>
	<tr><td>Macro Avg</td><td>0.95</td><td>0.96</td><td>0.96</td></tr>
	<tr><td>Weighted Avg</td><td>0.96</td><td>0.96</td><td>0.96</td></tr>
</table>
</p>
<p>
Table 6 Performance of FASTTEXT on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.97</td><td>0.97</td><td>0.97</td></tr>
	<tr><td>LAB</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
	<tr><td>Macro Avg</td><td>0.98</td><td>0.98</td><td>0.98</td></tr>
	<tr><td>Weighted Avg</td><td>0.98</td><td>0.98</td><td>0.98</td></tr>
</table>
</p>
<p>
Table 7. Performance of STANDARD GRU on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.97</td><td>0.94</td><td>0.95</td></tr>
	<tr><td>LAB</td><td>0.98</td><td>0.99</td><td>0.98</td></tr>
	<tr><td>Macro Avg</td><td>0.97</td><td>0.96</td><td>0.97</td></tr>
	<tr><td>Weighted Avg</td><td>0.97</td><td>0.97</td><td>0.97</td></tr>
</table>
</p>
<p>
Table 8. Performance of BiGRU on 2871 Samples
<table style="border: 1px solid black">
	<tr><td></td><td>Precision</td><td>Recall</td><td>F1-score</td></tr>
	<tr><td>DOC</td><td>0.98</td><td>0.98</td><td>0.98</td></tr>
	<tr><td>LAB</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
	<tr><td>Macro Avg</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
	<tr><td>Weighted Avg</td><td>0.99</td><td>0.99</td><td>0.99</td></tr>
</table>
</p>

<p><a href="#top">Top</a></p>
</div>


<div id='entity-detection'>
<h3>Entity Extraction through Conditional Random Fields model (CRF)</h3>
<p>
In health care industry, lab test sections are normally written in free text format. With the rapid expansion of health care claims, the demands for automatically extracting lab test information from raw messages give rise to the opportunities to train computers to automatically review, extract and aggregate information to make data products. To make terms meaningful and to extract them from the text, Named Entity Recognition (NER) is one of the most used techniques.
<br>
<br>
There are several challenges in lab entity extraction. Each test starts with test name followed by quantities, units, references, and dates. Test names are pre-defined medical terms that can be stored in a domain dictionary. Units can also be considered as a set of pre-defined terms. The test results and references have pre-defined formats that have the minimum values, the maximum values, separators, and parenthesis, for example “(1.2-2.5)”. The formats of the test dates are also pre-defined. Other than the characteristics of each individual token in test results, the format of the test section is also different from other sections. In the test section, each row is a lab test, and each column is a measurement that can be results, units, references, etc. Test results and references fields can have words in common. Also, test units and abnormal flags fields can have words in common. However, each field needs to be labeled separately. All the tests are listed together in separate lines. We want to model these characteristics of the lab sections and to extract lab entities from the doctor’s notes.

</p>

<h5>Sample Data</h5>

<table style="border: 1px solid black">
	<tr><td>Raw</td><td>Lab Test Name</td><td>Value</td><td>Unit</td><td>Reference Range</td><td>Abnormal Flag</td></tr>
	<tr><td>RBC, 4.02, 10E6/UL, (4.50-5.90), L</td><td>	RBC</td><td>['4.02']</td><td>['10E6/UL']</td><td>['(4.50-5.90)']</td><td>['L']</td></tr>
	<tr><td>GRAN, 0.1, 10E3/UL, (0.0-0.0), H</td><td>GRAN</td><td>['0.1']</td><td>['10E3/UL']</td><td>['(0.0-0.0)']</td><td>['H']</td></tr>
	<tr><td>BUN, 22, MG/DL, (6-20), H</td><td>BUN</td><td>['22']</td><td>['MG/DL']</td><td>['(6-20)']</td><td>['H']</td></tr>
	<tr><td>GLUCOSE, 135, MG/DL, (70-110), H</td><td>GLUCOSE</td><td>['135']</td><td>['MG/DL']</td><td>['(70-110)']</td><td>['H']</td></tr>
	<tr><td>C-REACTIVE, PROTEIN, 4.7, MG/DL, (0.5-1.0), H</td><td>C-REACTIVE PROTEIN</td><td>['4.7']</td><td>['MG/DL']</td><td>['(0.5-1.0)']</td><td>['H']</td></tr>
</table>

<h5>Performance</h5>
<table style="border: 1px solid black">
	<tr><td>Evaluation</td><td>Precision</td><td>Recall</td><td>F1-score</td><td>Support Samples</td></tr>
	<tr><td>LAB Test Name</td><td>1</td><td>1</td><td>1</td><td>27700</td></tr>
	<tr><td>Reference Range</td><td>1</td><td>1</td><td>1</td><td>1874</td></tr>
	<tr><td>Unit</td><td>1</td><td>1</td><td>1</td><td>884</td></tr>
	<tr><td>Abnormal Flag</td><td>1</td><td>1</td><td>1</td><td>1915</td></tr>
	<tr><td>Value</td><td>1</td><td>1</td><td>1</td><td>9510</td></tr>
	<tr><td>Macro Avg</td><td>1</td><td>1</td><td>1</td><td>41883</td></tr>
	<tr><td>Weighted Avg</td><td>1</td><td>1</td><td>1</td><td>41883</td></tr>
</table>

<p><a href="#top">Top</a></p>
</div>



<div id='fraud-detection'>
<h3>Credit Card Fraud Detection</h3>
<p>
In the banking industry, several applications are performed on a daily basis to ensure the smooth functioning of the industry. Customer profiling, risk detection, and fraud detection are some of the critical applications that are performed by banks to manage their operations efficiently and provide a secure and reliable service to their customers.
<br>
<br>
Customer profiling involves collecting and analyzing data about customers to understand their needs and preferences. This information helps banks to develop personalized products and services that meet the specific needs of their customers.
<br>
<br>
Risk detection involves identifying potential risks that can affect the financial health of the bank. Banks use advanced analytical techniques to identify and monitor risks related to credit, market, liquidity, operational, and reputational risks. This helps them to take proactive measures to mitigate these risks and ensure the stability of their operations.

Fraud detection involves identifying and preventing fraudulent activities that can harm the bank and its customers. Banks use sophisticated fraud detection techniques to identify patterns and anomalies in customer transactions that may indicate fraudulent activities. This helps them to prevent financial losses and protect the interests of their customers.
<br>
<br>
Credit card fraud detection (CCFD) is like looking for needles in a haystack. It requires finding, out of millions of daily transactions, which ones are fraudulent. Due to the ever-increasing amount of data, it is now almost impossible for a human specialist to detect meaningful patterns from transaction data. For this reason, the use of machine learning techniques is now widespread in the field of fraud detection, where information extraction from large datasets is required. 
<br>
<br>
In the simplest form, a payment card transaction consists of any amount paid to a merchant by a customer at a certain time. For fraud detection, it is also generally assumed that the legitimacy of all transactions is known as either genuine or fraudulent. This is usually represented by a binary label, with a value of 0 for a genuine transaction, and a value of 1 for fraudulent transactions.
<br>
<br>
In credit card fraud detection, data typically consists of transaction data, collected, for example, by a payment processor or a bank. Transaction data can be divided into three groups:
<ul>
<li>Account-related features: They include for example the account number, the date of the account opening, the card limit, the card expiry date, etc.</li>
<li>Transaction-related features: They include for example the transaction reference number, the account number, the transaction amount, the terminal (i.e., POS) number, the transaction time, etc. From the terminal, one can also obtain an additional category of information: merchant-related features such as its category code (restaurant, supermarket, …) or its location.</li>
<li>Customer-related features: They include for example the customer number, the type of customer (low profile, high profile, …), etc.</li>
</ul>
The challenges in credit card fraud detection include class imbalance, concept drift, near real-time requirements, categorical features, sequential modeling, class overlap, performance measurements, and lack of public datasets. We provide solution to balance normal/fraud samples and evaluate the solution with Logistic Regression, Decision Tree with depth of two, Decision Tree with unlimited depth, Random Forest, and XGBoost.
</p>

<h5>Sample Data</h5>
<table style="border: 1px solid black">
	<tr><td>Features</td><td>Values</td></tr>
	<tr><td>TX_AMOUNT</td><td>57.12</td></tr>
	<tr><td>TX_DURING_WEEKEND</td><td>1.0</td></tr>
	<tr><td>TX_DURING_NIGHT</td><td>1.0</td></tr>
	<tr><td>CUSTOMER_ID_NB_TX_1DAY_WINDOW</td><td>1.0</td></tr>
	<tr><td>CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW</td><td>57.12</td></tr>
	<tr><td>CUSTOMER_ID_NB_TX_7DAY_WINDOW</td><td>1.0</td></tr>
	<tr><td>CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW</td><td>57.12</td></tr>
	<tr><td>CUSTOMER_ID_NB_TX_30DAY_WINDOW</td><td>1.0</td></tr>
	<tr><td>CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW</td><td>57.12</td></tr>
	<tr><td>TERMINAL_ID_NB_TX_1DAY_WINDOW</td><td>0.0</td></tr>
	<tr><td>TERMINAL_ID_RISK_1DAY_WINDOW</td><td>0.0</td></tr>
	<tr><td>TERMINAL_ID_NB_TX_7DAY_WINDOW</td><td>0</td></tr>
	<tr><td>TERMINAL_ID_RISK_7DAY_WINDOW</td><td>0.0</td></tr>
	<tr><td>TERMINAL_ID_NB_TX_30DAY_WINDOW</td><td>0.0</td></tr>
	<tr><td>TERMINAL_ID_RISK_30DAY_WINDOW</td><td>0.0</td></tr>
	<tr><td>TX_FRAUD</td><td>1</td></tr>
</table>

<h5>Performance</h5>
<table style="border: 1px solid black">
	<tr><td></td><td>AUC ROC</td><td>Average Precision</td><td>Card Precision@100</td></tr>
	<tr><td>Logistic Regression</td><td>0.644</td><td>0.126</td><td>0.181</td></tr>
	<tr><td>Decision Tree with Depth=2</td><td>0.829</td><td>0.246</td><td>0.215</td></tr>
	<tr><td>Decision Tree with Unlimited Depth</td><td>0.915</td><td>0.576</td><td>0.222</td></tr>
	<tr><td>Random Forest</td><td>0.991</td><td>0.944</td><td>0.223</td></tr>
	<tr><td>XGBoost</td><td>0.949</td><td>0.810</td><td>0.134</td></tr>
</table>

<p><a href="#top">Top</a></p>

</div>


							</article>
						<!-- About -->
							<article id="about">
								<h2 class="major">About</h2>
								<span class="image main"><img src="images/pic03.jpg" alt="" /></span>
								<p>Lucy Lu is a data scientist. Her research interest is in Insights Analysis, Natural Language Processing, Entity Resolution, Deep Learning and Image Processing. She has been providing machine learning solutions in different research areas. Her research solutions have won best paper awards for three times. With her experiences in technical details, her solutions can achieve almost perfect results.</p>
							</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Lucy Lu, Ph.D.</label>
										</div>
										<div class="field half">
											<label for="email">mollielu2012@gmail.com</label>
										</div>
									</div>
								</form>
								<ul class="icons">
									<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</article>


					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Design: <a href="https://html5up.net">Lucy Lu</a>.</p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
